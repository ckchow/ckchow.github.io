---
layout: post
title: Multiplatform conda in 2024
---

I have had to spin up multiple ML projects recently that have to run on both apple silicon and on CUDA. This situation is mostly
due to my own development preferences (I like to use my macbook and to not have to be at my desktop all the time), but 
it's not immediately obvious from Google searches what the best way is to do this.

# Why Conda
But first, before describing how to accomplish the thing described in the title of this post, I want to go over again --
mostly to make concrete to myself -- the reasons why one might need to use Conda. The major reason is mostly **CUDA**.

## You usually have to "bring your own CUDA"
There are [currently 12](https://en.wikipedia.org/wiki/CUDA#GPUs_supported) CUDA SDK versions in the world. Libraries
and application code typically target a specific SDK version (whatever is popular when they are written), and typically
don't bother with updates for new SDKs that come out. There is currently no way I know of to manage multiple CUDA SDK installs,
and so any code you use will try to dynamically load the SDK that the system has installed (if any) and potentially break
or work in an unexpected fashion.

## Using system tools that you can't install
The main tool here is probably a specific GCC version, though you may also run into the need for specific versions
of MESA, OpenGL, etc. This is especially relevant when you are working with robotics or RL libraries that are
written without portability or maintenance in mind. If you're on a cluster, you probably don't have root
permissions and can't install


# Why vanilla Conda doesn't work
- Pinning requires use of a third party tool
- Conda is difficult to install in a nonprivileged environment, you might be forced to use the system version and then
  you have to deal with...
    - The default conda dependency solver is incredibly slow.
